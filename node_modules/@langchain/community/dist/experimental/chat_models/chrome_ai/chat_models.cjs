"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.ChatChromeAI = void 0;
const chat_models_1 = require("@langchain/core/language_models/chat_models");
const messages_1 = require("@langchain/core/messages");
const outputs_1 = require("@langchain/core/outputs");
const stream_1 = require("@langchain/core/utils/stream");
function formatPrompt(messages) {
    return messages
        .map((message) => {
        if (typeof message.content !== "string") {
            throw new Error("ChatChromeAI does not support non-string message content.");
        }
        return `${message._getType()}: ${message.content}`;
    })
        .join("\n");
}
/**
 * To use this model you need to have the `Built-in AI Early Preview Program`
 * for Chrome. You can find more information about the program here:
 * @link https://developer.chrome.com/docs/ai/built-in
 *
 * @example
 * ```typescript
 * // Initialize the ChatChromeAI model.
 * const model = new ChatChromeAI({
 *   temperature: 0.5, // Optional. Default is 0.5.
 *   topK: 40, // Optional. Default is 40.
 * });
 *
 * // Call the model with a message and await the response.
 * const response = await model.invoke([
 *   new HumanMessage({ content: "My name is John." }),
 * ]);
 * ```
 */
class ChatChromeAI extends chat_models_1.SimpleChatModel {
    static lc_name() {
        return "ChatChromeAI";
    }
    constructor(inputs) {
        super({
            callbacks: {},
            ...inputs,
        });
        Object.defineProperty(this, "session", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 0.5
        });
        Object.defineProperty(this, "topK", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 40
        });
        Object.defineProperty(this, "promptFormatter", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        this.temperature = inputs?.temperature ?? this.temperature;
        this.topK = inputs?.topK ?? this.topK;
        this.promptFormatter = inputs?.promptFormatter ?? formatPrompt;
    }
    _llmType() {
        return "chrome-ai";
    }
    /**
     * Initialize the model. This method must be called before calling `.invoke()`.
     */
    async initialize() {
        if (typeof window === "undefined") {
            throw new Error("ChatChromeAI can only be used in the browser.");
        }
        const { ai } = window;
        const canCreateTextSession = await ai.canCreateTextSession();
        if (canCreateTextSession === "no" /* AIModelAvailability.No */) {
            throw new Error("The AI model is not available.");
        }
        else if (canCreateTextSession === "after-download" /* AIModelAvailability.AfterDownload */) {
            throw new Error("The AI model is not yet downloaded.");
        }
        this.session = await ai.createTextSession({
            topK: this.topK,
            temperature: this.temperature,
        });
    }
    /**
     * Call `.destroy()` to free resources if you no longer need a session.
     * When a session is destroyed, it can no longer be used, and any ongoing
     * execution will be aborted. You may want to keep the session around if
     * you intend to prompt the model often since creating a session can take
     * some time.
     */
    destroy() {
        if (!this.session) {
            return console.log("No session found. Returning.");
        }
        this.session.destroy();
    }
    async *_streamResponseChunks(messages, _options, runManager) {
        if (!this.session) {
            throw new Error("Session not found. Please call `.initialize()` first.");
        }
        const textPrompt = this.promptFormatter(messages);
        const stream = this.session.promptStreaming(textPrompt);
        const iterableStream = stream_1.IterableReadableStream.fromReadableStream(stream);
        let previousContent = "";
        for await (const chunk of iterableStream) {
            const newContent = chunk.slice(previousContent.length);
            previousContent += newContent;
            yield new outputs_1.ChatGenerationChunk({
                text: newContent,
                message: new messages_1.AIMessageChunk({
                    content: newContent,
                    additional_kwargs: {},
                }),
            });
            await runManager?.handleLLMNewToken(newContent);
        }
    }
    async _call(messages, options, runManager) {
        const chunks = [];
        for await (const chunk of this._streamResponseChunks(messages, options, runManager)) {
            chunks.push(chunk.text);
        }
        return chunks.join("");
    }
}
exports.ChatChromeAI = ChatChromeAI;
