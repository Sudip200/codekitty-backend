import { SimpleChatModel, type BaseChatModelParams } from "@langchain/core/language_models/chat_models";
import type { BaseLanguageModelCallOptions } from "@langchain/core/language_models/base";
import { CallbackManagerForLLMRun } from "@langchain/core/callbacks/manager";
import { BaseMessage } from "@langchain/core/messages";
import { ChatGenerationChunk } from "@langchain/core/outputs";
export interface AI {
    canCreateTextSession(): Promise<AIModelAvailability>;
    createTextSession(options?: AITextSessionOptions): Promise<AITextSession>;
    defaultTextSessionOptions(): Promise<AITextSessionOptions>;
}
export interface AITextSession {
    prompt(input: string): Promise<string>;
    promptStreaming(input: string): ReadableStream;
    destroy(): void;
    clone(): AITextSession;
}
export interface AITextSessionOptions {
    topK: number;
    temperature: number;
}
export declare const enum AIModelAvailability {
    Readily = "readily",
    AfterDownload = "after-download",
    No = "no"
}
export interface ChromeAIInputs extends BaseChatModelParams {
    topK?: number;
    temperature?: number;
    /**
     * An optional function to format the prompt before sending it to the model.
     */
    promptFormatter?: (messages: BaseMessage[]) => string;
}
export interface ChromeAICallOptions extends BaseLanguageModelCallOptions {
}
/**
 * To use this model you need to have the `Built-in AI Early Preview Program`
 * for Chrome. You can find more information about the program here:
 * @link https://developer.chrome.com/docs/ai/built-in
 *
 * @example
 * ```typescript
 * // Initialize the ChatChromeAI model.
 * const model = new ChatChromeAI({
 *   temperature: 0.5, // Optional. Default is 0.5.
 *   topK: 40, // Optional. Default is 40.
 * });
 *
 * // Call the model with a message and await the response.
 * const response = await model.invoke([
 *   new HumanMessage({ content: "My name is John." }),
 * ]);
 * ```
 */
export declare class ChatChromeAI extends SimpleChatModel<ChromeAICallOptions> {
    session?: AITextSession;
    temperature: number;
    topK: number;
    promptFormatter: (messages: BaseMessage[]) => string;
    static lc_name(): string;
    constructor(inputs?: ChromeAIInputs);
    _llmType(): string;
    /**
     * Initialize the model. This method must be called before calling `.invoke()`.
     */
    initialize(): Promise<void>;
    /**
     * Call `.destroy()` to free resources if you no longer need a session.
     * When a session is destroyed, it can no longer be used, and any ongoing
     * execution will be aborted. You may want to keep the session around if
     * you intend to prompt the model often since creating a session can take
     * some time.
     */
    destroy(): void;
    _streamResponseChunks(messages: BaseMessage[], _options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): AsyncGenerator<ChatGenerationChunk>;
    _call(messages: BaseMessage[], options: this["ParsedCallOptions"], runManager?: CallbackManagerForLLMRun): Promise<string>;
}
